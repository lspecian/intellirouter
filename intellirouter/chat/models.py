from typing import Dict, List, Any, Optional, Union, Literal
from pydantic import BaseModel, Field

class ChatMessage(BaseModel):
    """
    A message in a chat conversation.
    
    Args:
        role: The role of the message sender. Can be "system", "user", "assistant", "function", or "tool".
        content: The content of the message.
        name: Optional name of the sender. Required for function and tool roles.
        function_call: Optional function call information.
        tool_calls: Optional tool call information.
    """
    role: Literal["system", "user", "assistant", "function", "tool"]
    content: Optional[str] = None
    name: Optional[str] = None
    function_call: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None

class ChatCompletionRequest(BaseModel):
    """
    Request for a chat completion.
    
    Args:
        model: The model to use for the completion.
        messages: The messages to generate a completion for.
        temperature: Controls randomness. Higher values (e.g., 0.8) make output more random, lower values (e.g., 0.2) make it more deterministic.
        top_p: Controls diversity via nucleus sampling. 0.1 means only tokens with the top 10% probability mass are considered.
        n: How many completions to generate for each prompt.
        stream: Whether to stream the response.
        stop: Up to 4 sequences where the API will stop generating further tokens.
        max_tokens: The maximum number of tokens to generate.
        presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far.
        frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their frequency in the text so far.
        logit_bias: Modify the likelihood of specified tokens appearing in the completion.
        user: A unique identifier representing your end-user.
    """
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    n: Optional[int] = None
    stream: Optional[bool] = None
    stop: Optional[Union[str, List[str]]] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    """
    A choice in a chat completion.
    
    Args:
        index: The index of the choice.
        message: The message generated by the model.
        finish_reason: The reason the model stopped generating tokens.
    """
    index: int
    message: ChatMessage
    finish_reason: Optional[str] = None

class ChatCompletion(BaseModel):
    """
    A chat completion.
    
    Args:
        id: The ID of the completion.
        object: The object type.
        created: The Unix timestamp of when the completion was created.
        model: The model used for the completion.
        choices: The completion choices.
        usage: The token usage information.
    """
    id: str
    object: str
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Dict[str, int]

class ChatCompletionChunkDelta(BaseModel):
    """
    A delta in a chat completion chunk.
    
    Args:
        role: The role of the message sender.
        content: The content of the message.
        function_call: Optional function call information.
        tool_calls: Optional tool call information.
    """
    role: Optional[str] = None
    content: Optional[str] = None
    function_call: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None

class ChatCompletionChunkChoice(BaseModel):
    """
    A choice in a chat completion chunk.
    
    Args:
        index: The index of the choice.
        delta: The delta of the message.
        finish_reason: The reason the model stopped generating tokens.
    """
    index: int
    delta: ChatCompletionChunkDelta
    finish_reason: Optional[str] = None

class ChatCompletionChunk(BaseModel):
    """
    A chunk of a streaming chat completion.
    
    Args:
        id: The ID of the completion.
        object: The object type.
        created: The Unix timestamp of when the completion was created.
        model: The model used for the completion.
        choices: The completion choices.
    """
    id: str
    object: str
    created: int
    model: str
    choices: List[ChatCompletionChunkChoice]